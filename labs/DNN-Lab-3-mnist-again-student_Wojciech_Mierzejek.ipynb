{"cells":[{"cell_type":"markdown","metadata":{"id":"84VetyCaGLyR"},"source":["\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","<center><img src='https://drive.google.com/uc?id=1_utx_ZGclmCwNttSe40kYA6VHzNocdET' height=\"60\"></center>\n","\n","AI TECH - Akademia Innowacyjnych Zastosowań Technologii Cyfrowych. Program Operacyjny Polska Cyfrowa na lata 2014-2020\n","<hr>\n","\n","<center><img src='https://drive.google.com/uc?id=1BXZ0u3562N_MqCLcekI-Ens77Kk4LpPm'></center>\n","\n","<center>\n","Projekt współfinansowany ze środków Unii Europejskiej w ramach Europejskiego Funduszu Rozwoju Regionalnego\n","Program Operacyjny Polska Cyfrowa na lata 2014-2020,\n","Oś Priorytetowa nr 3 \"Cyfrowe kompetencje społeczeństwa\" Działanie  nr 3.2 \"Innowacyjne rozwiązania na rzecz aktywizacji cyfrowej\"\n","Tytuł projektu:  „Akademia Innowacyjnych Zastosowań Technologii Cyfrowych (AI Tech)”\n","    </center>"]},{"cell_type":"markdown","metadata":{"id":"ziZ9i7tXbO1T"},"source":["In this lab, you will implement some of the techniques discussed in the lecture.\n","\n","Below you are given a solution to the previous scenario. It has two serious drawbacks:\n"," * The output predictions do not sum up to one (i.e. the output is not a probability distribution), even though the images always contain exactly one digit.\n"," * It uses MSE coupled with output sigmoid, which can lead to saturation and slow convergence.\n","\n","**Task 0.** Implement a numerically stable version of softmax.\n","\n","**Task 1.** Use softmax instead of coordinate-wise sigmoid and use log-loss instead of MSE. Test to see if this improves convergence. Hint: When implementing backprop it might be easier to consider these two functions as a single block, rather than compute the gradient over the softmax values.\n","\n","**Task 2.** Implement L2 regularization and add momentum to the SGD algorithm. Play with different amounts of regularization and momentum. See if this improves accuracy/convergence.\n","\n","**Task 3 (optional).** Implement Adagrad or AdamW (currently popular in LLM training), dropout, and some simple data augmentations (e.g. tiny rotations/shifts, etc.). Again, test to see how these changes improve accuracy/convergence.\n","\n","**Task 4.** Try adding extra layers to the network. Again, test how the changes you introduced affect accuracy/convergence. As a start, you can try this architecture: [784,100,30,10]\n","\n","The provided model evaluation code (`evaluate_model`) may take some time to complete. During implementation, you can change the number of evaluated models to 1 and reduce the number of tested learning rates, and epochs.  \n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"iZypYewcXywA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761749284348,"user_tz":-60,"elapsed":12720,"user":{"displayName":"Wojciech Mierzejek","userId":"03526477555098188127"}},"outputId":"f033d64c-2ed6-4624-ba7c-0298be98d8f2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n","Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n","2025-10-29 14:48:04 URL:https://s3.amazonaws.com/img-datasets/mnist.npz [11490434/11490434] -> \"mnist.npz\" [1]\n"]}],"source":["!pip install tqdm pandas\n","!wget --no-verbose -O mnist.npz https://s3.amazonaws.com/img-datasets/mnist.npz"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"P22HqX9AbO1a","executionInfo":{"status":"ok","timestamp":1761749300326,"user_tz":-60,"elapsed":15971,"user":{"displayName":"Wojciech Mierzejek","userId":"03526477555098188127"}}},"outputs":[],"source":["import random\n","import json\n","from pathlib import Path\n","from typing import Any, Callable, Sequence\n","\n","import numpy as np\n","import pandas as pd\n","from IPython.display import display\n","from numpy.typing import NDArray\n","from torchvision import datasets, transforms\n","from tqdm import tqdm\n","\n","FloatNDArray = NDArray[np.float64]\n","\n","np.random.seed(42)"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"N9jGPaZhbO2B","executionInfo":{"status":"ok","timestamp":1761749300939,"user_tz":-60,"elapsed":606,"user":{"displayName":"Wojciech Mierzejek","userId":"03526477555098188127"}}},"outputs":[],"source":["def load_mnist(\n","    path: Path = Path(\"mnist.npz\")\n",") -> tuple[FloatNDArray, FloatNDArray, FloatNDArray, FloatNDArray]:\n","    \"\"\"\n","    Load the MNIST dataset (grayscale 28 x 28 images of hand-written digits).\n","\n","    Returns tuple of:\n","    - x_train: shape (N_train, H * W), grayscale values 0..1.\n","    - y_train: shape (N_train, 10), one-hot-encoded label, dtype float64.\n","    - x_test: shape (N_test, H * W), grayscale values 0..1.\n","    - y_train: shape (N_test, 10), one-hot-encoded label, dtype float64.\n","\n","    More: https://en.wikipedia.org/wiki/MNIST_database\n","    \"\"\"\n","    with np.load(path) as f:\n","        x_train, _y_train = f[\"x_train\"], f[\"y_train\"]\n","        x_test, _y_test = f[\"x_test\"], f[\"y_test\"]\n","\n","    H = W = 28\n","    N_train = len(x_train)\n","    N_test = len(x_test)\n","    assert x_train.shape == (N_train, H, W) and _y_train.shape == (N_train,)\n","    assert x_test.shape == (N_test, H, W) and _y_test.shape == (N_test,)\n","\n","    x_train = x_train.reshape(N_train, H * W) / 255.0\n","    x_test = x_test.reshape(N_test, H * W) / 255.0\n","\n","    y_train = np.zeros((N_train, 10), dtype=np.float64)\n","    y_train[np.arange(N_train), _y_train] = 1\n","\n","    y_test = np.zeros((N_test, 10))\n","    y_test[np.arange(N_test), _y_test] = 1\n","\n","    return x_train, y_train, x_test, y_test\n","\n","x_train, y_train, x_test, y_test = load_mnist()"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"w3gAyqw4bO1p","executionInfo":{"status":"ok","timestamp":1761749300950,"user_tz":-60,"elapsed":4,"user":{"displayName":"Wojciech Mierzejek","userId":"03526477555098188127"}}},"outputs":[],"source":["def sigmoid(z: FloatNDArray) -> FloatNDArray:\n","    return 1.0 / (1.0 + np.exp(-z))\n","\n","\n","def sigmoid_prime(z: FloatNDArray) -> FloatNDArray:\n","    \"\"\"Derivative of the sigmoid function.\"\"\"\n","    return sigmoid(z) * (1 - sigmoid(z))"]},{"cell_type":"markdown","metadata":{"id":"CvVG90fCXywB"},"source":["## Warm-Up\n","Implement a numerically stable version of softmax.  \n","\n","In general, softmax is defined as  \n","$$\\text{softmax}(x_1, x_2, \\ldots, x_n) = (\\frac{e^{x_1}}{\\sum_i{e^{x_i}}}, \\frac{e^{x_2}}{\\sum_i{e^{x_i}}}, \\ldots, \\frac{e^{x_n}}{\\sum_i{e^{x_i}}})$$  \n","However, taking $e^{1000000}$ can result in NaN.  \n","Can you implement softmax so that the highest power to which e will be risen will be at most $0$ and the predictions will be mathematically equivalent?  \n","\n","Hint: <sub><sub><sub>sǝnlɐʌ llɐ ɯoɹɟ ʇᴉ ʇɔɐɹʇqns  puɐ ᴉ‾x ʇsǝƃɹɐl ǝɥʇ ǝʞɐʇ</sub></sub></sub>"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"_rutQhoaXywC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761749300969,"user_tz":-60,"elapsed":15,"user":{"displayName":"Wojciech Mierzejek","userId":"03526477555098188127"}},"outputId":"a43e8c24-624d-459b-8d73-db954409f703"},"outputs":[{"output_type":"stream","name":"stdout","text":["OK\n"]}],"source":["def unstable_softmax(x: FloatNDArray, axis: int = -1) -> FloatNDArray:\n","    e = np.exp(x)\n","    return e / np.sum(e, axis=axis, keepdims=True)\n","\n","\n","def stable_softmax(x: FloatNDArray, axis: int = -1) -> FloatNDArray:\n","    x_max = np.max(x, axis=axis, keepdims=True)\n","    e = np.exp(x - x_max)\n","    return e / np.sum(e, axis=axis, keepdims=True)\n","\n","\n","### TESTS ###\n","def _test_one(x: FloatNDArray, y: FloatNDArray) -> None:\n","    r = stable_softmax(x)\n","    assert r.shape == y.shape, f\"Expected shape {y.shape}, got {r.shape=}\"\n","    assert np.isclose(np.ones(x.shape[0]), r.sum(axis=-1), atol=1e-5, rtol=0).all()\n","    assert np.isclose(y, r, atol=1e-5, rtol=0).all()\n","\n","def test_stable_softmax() -> None:\n","    x1 = np.random.rand(100, 32).astype(np.float64)\n","    _test_one(x1, unstable_softmax(x1))\n","\n","    x2 = np.ones((10, 10, 32), dtype=np.float64) * 1e6\n","    _test_one(x2, np.ones_like(x2) / x2.shape[-1])\n","\n","    print(\"OK\")\n","\n","test_stable_softmax()"]},{"cell_type":"markdown","source":["## ModelResults\n"],"metadata":{"id":"WWFnNWF8fWJB"}},{"cell_type":"code","execution_count":6,"metadata":{"id":"FgEA2XRRbO2X","executionInfo":{"status":"ok","timestamp":1761749300979,"user_tz":-60,"elapsed":4,"user":{"displayName":"Wojciech Mierzejek","userId":"03526477555098188127"}}},"outputs":[],"source":["class ModelResults:\n","    \"\"\"Just a helper class for gathering results in a nice table. Feel free to ignore.\"\"\"\n","    def __init__(self):\n","        # Map from model name to map from lr to list of test accuracies.\n","        self.results = dict[str, dict[float, list[float]]]()\n","\n","    def clear(self, model_name: str | None = None) -> None:\n","        \"\"\"Forget results for a given model (defaults to all models).\"\"\"\n","        if model_name:\n","            if model_name in self.results:\n","                del self.results[model_name]\n","        else:\n","            self.results = {}\n","\n","    def add_result(self, model_name: str, learning_rate: float, accuracy: float) -> None:\n","        if model_name not in self.results:\n","            self.results[model_name] = {}\n","        if learning_rate not in self.results[model_name]:\n","            self.results[model_name][learning_rate] = []\n","        self.results[model_name][learning_rate].append(accuracy)\n","\n","    def display_results(self) -> None:\n","        data = list[dict[str, Any]]()\n","        for model_name, model_results in self.results.items():\n","            for lr, accuracies in model_results.items():\n","                mean_accuracy = np.mean(accuracies)\n","                accuracy_summary = f\"{mean_accuracy:2.1%} ± {np.std(accuracies) * 100:.1f} p.p.\"\n","                data.append({\n","                    \"model\": model_name,\n","                    \"lr\": lr,\n","                    \"mean_accuracy\": mean_accuracy,\n","                    \"accuracy\": accuracy_summary\n","                })\n","\n","        df = pd.DataFrame(data).sort_values(\"mean_accuracy\", ascending=False)\n","        del df[\"mean_accuracy\"]\n","        display(df.style.format({\"lr\": \"{:.1g}\"}).hide())\n","\n","    def evaluate_model(\n","        self,\n","        model_name: str,\n","        model_constructor: Callable[[Sequence[int]], Any],\n","        layers: Sequence[int] = (784, 30, 10),\n","        learning_rates: Sequence[float] = (1.0, 10.0, 100.0),\n","        n_trainings: int = 3,\n","        **kwargs: Any\n","    ) -> None:\n","        # Automatic model name with parameters.\n","        if kwargs:\n","            if tuple(layers) != (784, 30, 10):\n","                model_name += \"[\" + \",\".join(str(n) for n in layers) + \"]\"\n","\n","            model_name += \"(\"\n","            for k, v in kwargs.items():\n","                if isinstance(v, (float,  np.floating)):\n","                    model_name += f\"{k}={v:.1g},\"\n","                else:\n","                    model_name += f\"{k}={v},\"\n","            model_name = model_name[:-1]\n","            model_name += \")\"\n","\n","        # Train for each learning rate, n_trainings times.\n","        for lr in learning_rates:\n","            print(f\"Checking {n_trainings} random trainings with with lr = {lr}\")\n","            for i in range(n_trainings):\n","                network = model_constructor(layers, **kwargs)\n","                accuracy = network.train(\n","                    (x_train, y_train),\n","                    epochs=10,\n","                    mini_batch_size=100,\n","                    learning_rate=lr,\n","                    test_data=(x_test, y_test),\n","                )\n","                self.add_result(model_name, lr, float(accuracy))\n","\n","\n","model_results = ModelResults()"]},{"cell_type":"markdown","source":["## Baseline\n","The solution to the previous lab: an MLP network with MSE loss on sigmoid outputs, trained with plain SGD (batched)."],"metadata":{"id":"kJD-EMvq6H2l"}},{"cell_type":"code","source":["class Network:\n","    def __init__(self, sizes: Sequence[int] = (784, 30, 10)):\n","        \"\"\"\n","        Args:\n","        - sizes: sequence of layer widths [N^0, ... , N^last]\n","          These are lengths of activation vectors, where:\n","          - N^0 is input size: H * W = 28 * 28 = 784.\n","          - N^last is the number of classes into which we can classify each input: 10.\n","        \"\"\"\n","        self.sizes = list(sizes)\n","\n","        # List of len(sizes) - 1 vectors of shape (N^1), (N^2), ..., (N^last).\n","        self.biases = [np.random.randn(n) for n in sizes[1:]]\n","\n","        # List of len(sizes) - 1 matrices of shape (N^i, N^{i-1}).\n","        # Weights are indexed by target node first.\n","        self.weights = [\n","            np.random.randn(n_out, n_in) / np.sqrt(n_in)\n","            for n_in, n_out in zip(sizes[:-1], sizes[1:], strict=True)\n","        ]\n","\n","        self.num_layers = len(self.weights)   # = len(sizes) - 1\n","\n","\n","    def feedforward(self, x: FloatNDArray) -> FloatNDArray:\n","        \"\"\"\n","        Run the network on a batch of cases of shape (B, N^0), values 0..1.\n","\n","        Returns last layer activations, shape (B, N^last), values 0..1.\n","        \"\"\"\n","        g = x\n","        for w, b in zip(self.weights, self.biases, strict=True):\n","            # Shapes (B, N^{i-1}) @ (N^{i-1}, N^i) + (N^i,)  ==  (B, N^i)\n","            g = sigmoid(g @ w.T + b)\n","        return g\n","\n","    def learning_step(self, x_mini_batch: FloatNDArray, y_mini_batch: FloatNDArray, learning_rate: float) -> None:\n","        \"\"\"\n","        Update network parameters with a single mini-batch step of backpropagation and gradient descent.\n","\n","        Args:\n","        - x_mini_batch: shape (B, N^0) where B is mini_batch_size.\n","        - y_mini_batch: shape (B, N^last).\n","        - learning_rate.\n","        \"\"\"\n","        grads_w, grads_b = self.backprop(x_mini_batch, y_mini_batch)\n","\n","        # Gradient descent step.\n","        self.weights = [\n","            w - learning_rate * grad_w\n","            for w, grad_w in zip(self.weights, grads_w, strict=True)\n","        ]\n","        self.biases = [\n","            b - learning_rate * grad_b\n","            for b, grad_b in zip(self.biases, grads_b, strict=True)\n","        ]\n","\n","    def backprop(\n","        self, x: FloatNDArray, y: FloatNDArray\n","    ) -> tuple[list[FloatNDArray], list[FloatNDArray]]:\n","        \"\"\"\n","        Backpropagation for a mini-batch (vectorized).\n","\n","        Args:\n","        - x: input, shape (B, N^0)\n","        - y: target label (one-hot encoded), shape (B, N^last)\n","\n","        Returns (grads_w, grads_b), where:\n","        - grads_w: list of gradients over weights (shape (N^i, N^{i-1})), for each layer.\n","        - grads_b: list of gradients over biases (shape (N^i)), for each layer.\n","        \"\"\"\n","        B, N0 = x.shape\n","        assert N0 == self.sizes[0]\n","\n","        # Go forward, remembering all activations.\n","\n","        # Values after activation function (including inputs to the first layer),\n","        # shapes (B, N^0), (B, N^1), ..., (B, N^last).\n","        gs: list[FloatNDArray] = [x]\n","\n","        # TODO\n","        g = x\n","        for w, b in zip(self.weights, self.biases, strict=True):\n","            # Multiply shape (N^i, N^{i-1}) by (N^{i-1}, B) and transpose result to (B, N^i).\n","            # Then add shape (N^i), which gets broadcasted to (B, N^i).\n","            f = (w @ g.T).T + b\n","            g = sigmoid(f)\n","            gs.append(g)\n","\n","        assert [g.shape for g in gs] == [(B, n) for n in self.sizes], \\\n","            f'Shape mismatch: {[g.shape for g in gs]} vs {[(B, n) for n in self.sizes]}'\n","\n","        # Now go backward from the final cost applying backpropagation.\n","        grad_g = self.cost_derivative(gs[-1], y)  # shape initially (B, N^last), then layer by layer.\n","\n","        grads_w = []  # shapes (N^last, N^{last-1}), ..., (N^1, N^0)\n","        grads_b = []  # shapes (N^last,), ..., (N^1,)\n","\n","        # TODO\n","        for w, prev_g, g in reversed(list(zip(self.weights, gs[:-1], gs[1:], strict=True))):\n","            grad_f = grad_g * g * (1 - g)  # Shape initially (B, N^last), then layer by layer.\n","            grads_w.append(np.matmul(grad_f.T, prev_g))  # Multiply shape (N^i, B) by (B, N^{i-1})).\n","            grads_b.append(np.sum(grad_f, axis=0))  # shape (N^i)\n","            grad_g = grad_f @ w  # Multiply shape (B, N^i) by (N^i, N^{i-1}).\n","\n","        grads_w.reverse()  # Now shapes (N^1, N^0), ..., (N^last, N^{last-1}).\n","        grads_b.reverse()  # Now shapes (N^1,) ..., (N^last,).\n","\n","        for grad_b, b in zip(grads_b, self.biases, strict=True):\n","            assert grad_b.shape == b.shape, f'Shape mismatch: {grad_b.shape=} but {b.shape=}'\n","        for grad_w, w in zip(grads_w, self.weights, strict=True):\n","            assert grad_w.shape == w.shape, f'Shape mismatch: {grad_w.shape=} but {w.shape=}'\n","\n","        return grads_w, grads_b\n","\n","\n","    def cost_derivative(self, a: FloatNDArray, y: FloatNDArray) -> FloatNDArray:\n","        \"\"\"\n","        Gradient of loss (MSE) over output activations.\n","\n","        Args:\n","        - a: output activations, shape (B, N^last).\n","        - y: target values (one-hot encoded labels), shape (B, N^last).\n","\n","        Returns gradients, shape (B, N^last).\n","        \"\"\"\n","        assert a.shape == y.shape, f\"Shape mismatch: {a.shape=} but {y.shape=}\"\n","        B, N_last = a.shape\n","        return (2 / (B * N_last)) * (a - y.astype(np.float64))\n","\n","    def evaluate(self, x_test_data: FloatNDArray, y_test_data: FloatNDArray) -> np.float64:\n","        \"\"\"\n","        Compute accuracy: the ratio of correct answers for test_data.\n","\n","        Args:\n","        - x_test_data: shape (B, N^0).\n","        - y_test_data: shape (B, N^last).\n","        \"\"\"\n","        predictions = np.argmax(self.feedforward(x_test_data), axis=1)\n","        targets = np.argmax(y_test_data, axis=1)\n","        return np.mean(predictions == targets)\n","\n","    def train(\n","        self,\n","        training_data: tuple[FloatNDArray, FloatNDArray],\n","        test_data: tuple[FloatNDArray, FloatNDArray] | None = None,\n","        epochs: int = 2,\n","        mini_batch_size: int = 100,\n","        learning_rate: float = 1.0\n","    ) -> np.float64:\n","        x_train, y_train = training_data\n","        progress_bar = tqdm(range(epochs), desc=\"Epoch\")\n","        for epoch in progress_bar:\n","            for i in range(x_train.shape[0] // mini_batch_size):\n","                i_begin = i * mini_batch_size\n","                i_end = (i + 1) * mini_batch_size\n","                self.learning_step(x_train[i_begin:i_end], y_train[i_begin:i_end], learning_rate)\n","            if test_data:\n","                x_test, y_test = test_data\n","                accuracy = self.evaluate(x_test, y_test)\n","                progress_bar.set_postfix_str(f\"Test accuracy: {accuracy * 100:.2f} %\")\n","\n","        if test_data:\n","            x_test, y_test = test_data\n","            return self.evaluate(x_test, y_test)\n","        else:\n","            return np.float64(-1)\n","\n","model_results.evaluate_model(model_name=\"Baseline\", model_constructor=Network, n_trainings=3)\n","model_results.display_results()"],"metadata":{"id":"tnBAMfMP6IRJ","colab":{"base_uri":"https://localhost:8080/","height":361},"executionInfo":{"status":"ok","timestamp":1761749402422,"user_tz":-60,"elapsed":101439,"user":{"displayName":"Wojciech Mierzejek","userId":"03526477555098188127"}},"outputId":"f33efd57-a709-43a8-a597-105a1746b89a"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Checking 3 random trainings with with lr = 1.0\n"]},{"output_type":"stream","name":"stderr","text":["Epoch: 100%|██████████| 10/10 [00:18<00:00,  1.82s/it, Test accuracy: 90.76 %]\n","Epoch: 100%|██████████| 10/10 [00:11<00:00,  1.17s/it, Test accuracy: 91.06 %]\n","Epoch: 100%|██████████| 10/10 [00:10<00:00,  1.08s/it, Test accuracy: 91.09 %]\n"]},{"output_type":"stream","name":"stdout","text":["Checking 3 random trainings with with lr = 10.0\n"]},{"output_type":"stream","name":"stderr","text":["Epoch: 100%|██████████| 10/10 [00:10<00:00,  1.07s/it, Test accuracy: 94.73 %]\n","Epoch: 100%|██████████| 10/10 [00:08<00:00,  1.22it/s, Test accuracy: 94.72 %]\n","Epoch: 100%|██████████| 10/10 [00:10<00:00,  1.07s/it, Test accuracy: 94.84 %]\n"]},{"output_type":"stream","name":"stdout","text":["Checking 3 random trainings with with lr = 100.0\n"]},{"output_type":"stream","name":"stderr","text":["Epoch: 100%|██████████| 10/10 [00:10<00:00,  1.07s/it, Test accuracy: 8.92 %]\n","Epoch: 100%|██████████| 10/10 [00:11<00:00,  1.15s/it, Test accuracy: 10.09 %]\n","Epoch: 100%|██████████| 10/10 [00:08<00:00,  1.22it/s, Test accuracy: 8.92 %]\n"]},{"output_type":"display_data","data":{"text/plain":["<pandas.io.formats.style.Styler at 0x7b34e803b950>"],"text/html":["<style type=\"text/css\">\n","</style>\n","<table id=\"T_35163\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th id=\"T_35163_level0_col0\" class=\"col_heading level0 col0\" >model</th>\n","      <th id=\"T_35163_level0_col1\" class=\"col_heading level0 col1\" >lr</th>\n","      <th id=\"T_35163_level0_col2\" class=\"col_heading level0 col2\" >accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td id=\"T_35163_row0_col0\" class=\"data row0 col0\" >Baseline</td>\n","      <td id=\"T_35163_row0_col1\" class=\"data row0 col1\" >1e+01</td>\n","      <td id=\"T_35163_row0_col2\" class=\"data row0 col2\" >94.8% ± 0.1 p.p.</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_35163_row1_col0\" class=\"data row1 col0\" >Baseline</td>\n","      <td id=\"T_35163_row1_col1\" class=\"data row1 col1\" >1</td>\n","      <td id=\"T_35163_row1_col2\" class=\"data row1 col2\" >91.0% ± 0.1 p.p.</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_35163_row2_col0\" class=\"data row2 col0\" >Baseline</td>\n","      <td id=\"T_35163_row2_col1\" class=\"data row2 col1\" >1e+02</td>\n","      <td id=\"T_35163_row2_col2\" class=\"data row2 col2\" >9.3% ± 0.6 p.p.</td>\n","    </tr>\n","  </tbody>\n","</table>\n"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"NpZIY72SXywD"},"source":["## Task 1: softmax & cross-entropy loss\n","Use softmax instead of coordinate-wise sigmoid and use negative-log-loss instead of MSE. Test to see if this improves convergence.   \n","\n","Hints:\n","* When implementing backprop it's easier to consider these two functions as a single block, skipping the computation of the gradient over the softmax values, and going directly to gradients over logits (last pre-activations).\n","* Softmax is only used after the last layer; previous layers (and their grad computations) can be unchanged.\n","* Remember to update the forward pass in both places.\n","* Loss for a mini-batch is the mean of losses for each dataitem in it, by convention.\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"77iGbiSDXywD","colab":{"base_uri":"https://localhost:8080/","height":455},"executionInfo":{"status":"ok","timestamp":1761749519085,"user_tz":-60,"elapsed":116670,"user":{"displayName":"Wojciech Mierzejek","userId":"03526477555098188127"}},"outputId":"bf59691a-52f7-4663-eb95-8e293cfff3bc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Checking 3 random trainings with with lr = 1.0\n"]},{"output_type":"stream","name":"stderr","text":["Epoch: 100%|██████████| 10/10 [00:10<00:00,  1.04s/it, Test accuracy: 91.87 %]\n","Epoch: 100%|██████████| 10/10 [00:11<00:00,  1.10s/it, Test accuracy: 93.32 %]\n","Epoch: 100%|██████████| 10/10 [00:11<00:00,  1.10s/it, Test accuracy: 93.28 %]\n"]},{"output_type":"stream","name":"stdout","text":["Checking 3 random trainings with with lr = 10.0\n"]},{"output_type":"stream","name":"stderr","text":["Epoch: 100%|██████████| 10/10 [00:10<00:00,  1.10s/it, Test accuracy: 95.89 %]\n","Epoch: 100%|██████████| 10/10 [00:11<00:00,  1.10s/it, Test accuracy: 11.35 %]\n","Epoch: 100%|██████████| 10/10 [00:22<00:00,  2.21s/it, Test accuracy: 95.77 %]\n"]},{"output_type":"stream","name":"stdout","text":["Checking 3 random trainings with with lr = 100.0\n"]},{"output_type":"stream","name":"stderr","text":["Epoch: 100%|██████████| 10/10 [00:10<00:00,  1.10s/it, Test accuracy: 10.10 %]\n","Epoch: 100%|██████████| 10/10 [00:15<00:00,  1.54s/it, Test accuracy: 10.10 %]\n","Epoch: 100%|██████████| 10/10 [00:12<00:00,  1.28s/it, Test accuracy: 8.92 %]\n"]},{"output_type":"display_data","data":{"text/plain":["<pandas.io.formats.style.Styler at 0x7b34f0a44d10>"],"text/html":["<style type=\"text/css\">\n","</style>\n","<table id=\"T_7d882\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th id=\"T_7d882_level0_col0\" class=\"col_heading level0 col0\" >model</th>\n","      <th id=\"T_7d882_level0_col1\" class=\"col_heading level0 col1\" >lr</th>\n","      <th id=\"T_7d882_level0_col2\" class=\"col_heading level0 col2\" >accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td id=\"T_7d882_row0_col0\" class=\"data row0 col0\" >Baseline</td>\n","      <td id=\"T_7d882_row0_col1\" class=\"data row0 col1\" >1e+01</td>\n","      <td id=\"T_7d882_row0_col2\" class=\"data row0 col2\" >94.8% ± 0.1 p.p.</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_7d882_row1_col0\" class=\"data row1 col0\" >SoftMax</td>\n","      <td id=\"T_7d882_row1_col1\" class=\"data row1 col1\" >1</td>\n","      <td id=\"T_7d882_row1_col2\" class=\"data row1 col2\" >92.8% ± 0.7 p.p.</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_7d882_row2_col0\" class=\"data row2 col0\" >Baseline</td>\n","      <td id=\"T_7d882_row2_col1\" class=\"data row2 col1\" >1</td>\n","      <td id=\"T_7d882_row2_col2\" class=\"data row2 col2\" >91.0% ± 0.1 p.p.</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_7d882_row3_col0\" class=\"data row3 col0\" >SoftMax</td>\n","      <td id=\"T_7d882_row3_col1\" class=\"data row3 col1\" >1e+01</td>\n","      <td id=\"T_7d882_row3_col2\" class=\"data row3 col2\" >67.7% ± 39.8 p.p.</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_7d882_row4_col0\" class=\"data row4 col0\" >SoftMax</td>\n","      <td id=\"T_7d882_row4_col1\" class=\"data row4 col1\" >1e+02</td>\n","      <td id=\"T_7d882_row4_col2\" class=\"data row4 col2\" >9.7% ± 0.6 p.p.</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_7d882_row5_col0\" class=\"data row5 col0\" >Baseline</td>\n","      <td id=\"T_7d882_row5_col1\" class=\"data row5 col1\" >1e+02</td>\n","      <td id=\"T_7d882_row5_col2\" class=\"data row5 col2\" >9.3% ± 0.6 p.p.</td>\n","    </tr>\n","  </tbody>\n","</table>\n"]},"metadata":{}}],"source":["class Task1(Network):\n","    def __init__(self, sizes: Sequence[int]):\n","        super().__init__(sizes=sizes)\n","\n","    def feedforward(self, x: FloatNDArray) -> FloatNDArray:\n","        g = x\n","        for i, (w, b) in enumerate(zip(self.weights, self.biases)):\n","            if i == len(self.weights)-1:\n","                g = stable_softmax(g @ w.T + b)\n","            else:\n","                g = sigmoid(g @ w.T + b)\n","        return g\n","\n","    def backprop(\n","        self, x: FloatNDArray, y: FloatNDArray\n","    ) -> tuple[list[FloatNDArray], list[FloatNDArray]]:\n","        B, N0 = x.shape\n","        assert N0 == self.sizes[0]\n","\n","        # Forward pass.\n","        # Activations (including input) of shapes (B, N^0), (B, N^1), ..., (B, N^last).\n","        gs: list[FloatNDArray] = [x]\n","        g = x\n","        for i, (w, b) in enumerate(zip(self.weights, self.biases)):\n","            f = (w @ g.T).T + b\n","            if i == len(self.weights)-1:\n","                g = stable_softmax(f)\n","            else:\n","                g = sigmoid(f)\n","            gs.append(g)\n","\n","\n","        grads_w = []  # Shapes (N^last, N^{last-1}), ..., (N^1, N^0).\n","        grads_b = []  # Shapes (N^last,), ..., (N^1,).\n","\n","\n","        def cross_entropy_loss_derivative(a, y):\n","            B = a.shape[0]\n","            return (a-y) / B\n","\n","\n","\n","        # Backward pass.\n","        grad_g = cross_entropy_loss_derivative(gs[-1], y)\n","        for w, prev_g, g in reversed(list(zip(self.weights, gs[:-1], gs[1:], strict=True))):\n","            grad_f = grad_g * g * (1-g)\n","            grads_w.append(np.matmul(grad_f.T, prev_g))\n","            grads_b.append(np.sum(grad_f, axis=0))\n","            grad_g = grad_f @ w\n","\n","\n","        grads_w.reverse()  # Now shapes (N^1, N^0), ..., (N^last, N^{last-1}).\n","        grads_b.reverse()  # Now shapes (N^1,) ..., (N^last,).\n","        ###}\n","\n","        for grad_b, b in zip(grads_b, self.biases, strict=True):\n","            assert grad_b.shape == b.shape, f\"Shape mismatch: {grad_b.shape=} but {b.shape=}\"\n","        for grad_w, w in zip(grads_w, self.weights, strict=True):\n","            assert grad_w.shape == w.shape, f\"Shape mismatch: {grad_w.shape=} but {w.shape=}\"\n","\n","        return grads_w, grads_b\n","\n","\n","model_results.evaluate_model(model_name=\"SoftMax\", model_constructor=Task1, n_trainings=3)\n","model_results.display_results()"]},{"cell_type":"markdown","metadata":{"id":"FvIk4RxTXywD"},"source":["## Task 2: L2-regularization and momentum\n","Implement L2-regularization and add momentum to the SGD algorithm. Play with different amounts of regularization and momentum. See if this improves accuracy/convergence.  \n","A few notes:\n","* do not regularize the biases\n","* you can see an example pseudocode here [pytorch.org/docs/stable/generated/torch.optim.SGD.html](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"s3-03midXywD","colab":{"base_uri":"https://localhost:8080/","height":305},"executionInfo":{"status":"ok","timestamp":1761749532286,"user_tz":-60,"elapsed":13199,"user":{"displayName":"Wojciech Mierzejek","userId":"03526477555098188127"}},"outputId":"b88706ca-44d1-480b-d0c9-b4ba1fce6037"},"outputs":[{"output_type":"stream","name":"stdout","text":["Checking 1 random trainings with with lr = 10.0\n"]},{"output_type":"stream","name":"stderr","text":["Epoch: 100%|██████████| 10/10 [00:13<00:00,  1.31s/it, Test accuracy: 95.74 %]\n"]},{"output_type":"display_data","data":{"text/plain":["<pandas.io.formats.style.Styler at 0x7b35fceb4f80>"],"text/html":["<style type=\"text/css\">\n","</style>\n","<table id=\"T_08478\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th id=\"T_08478_level0_col0\" class=\"col_heading level0 col0\" >model</th>\n","      <th id=\"T_08478_level0_col1\" class=\"col_heading level0 col1\" >lr</th>\n","      <th id=\"T_08478_level0_col2\" class=\"col_heading level0 col2\" >accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td id=\"T_08478_row0_col0\" class=\"data row0 col0\" >L2&Momentum(l2_factor=1e-07,momentum=0.3)</td>\n","      <td id=\"T_08478_row0_col1\" class=\"data row0 col1\" >1e+01</td>\n","      <td id=\"T_08478_row0_col2\" class=\"data row0 col2\" >95.7% ± 0.0 p.p.</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_08478_row1_col0\" class=\"data row1 col0\" >Baseline</td>\n","      <td id=\"T_08478_row1_col1\" class=\"data row1 col1\" >1e+01</td>\n","      <td id=\"T_08478_row1_col2\" class=\"data row1 col2\" >94.8% ± 0.1 p.p.</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_08478_row2_col0\" class=\"data row2 col0\" >SoftMax</td>\n","      <td id=\"T_08478_row2_col1\" class=\"data row2 col1\" >1</td>\n","      <td id=\"T_08478_row2_col2\" class=\"data row2 col2\" >92.8% ± 0.7 p.p.</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_08478_row3_col0\" class=\"data row3 col0\" >Baseline</td>\n","      <td id=\"T_08478_row3_col1\" class=\"data row3 col1\" >1</td>\n","      <td id=\"T_08478_row3_col2\" class=\"data row3 col2\" >91.0% ± 0.1 p.p.</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_08478_row4_col0\" class=\"data row4 col0\" >SoftMax</td>\n","      <td id=\"T_08478_row4_col1\" class=\"data row4 col1\" >1e+01</td>\n","      <td id=\"T_08478_row4_col2\" class=\"data row4 col2\" >67.7% ± 39.8 p.p.</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_08478_row5_col0\" class=\"data row5 col0\" >SoftMax</td>\n","      <td id=\"T_08478_row5_col1\" class=\"data row5 col1\" >1e+02</td>\n","      <td id=\"T_08478_row5_col2\" class=\"data row5 col2\" >9.7% ± 0.6 p.p.</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_08478_row6_col0\" class=\"data row6 col0\" >Baseline</td>\n","      <td id=\"T_08478_row6_col1\" class=\"data row6 col1\" >1e+02</td>\n","      <td id=\"T_08478_row6_col2\" class=\"data row6 col2\" >9.3% ± 0.6 p.p.</td>\n","    </tr>\n","  </tbody>\n","</table>\n"]},"metadata":{}}],"source":["class Task2(Network):\n","    def __init__(\n","        self, sizes: Sequence[int], l2_factor: float = 1e-5, momentum: float = 0.2\n","    ):\n","        super().__init__(sizes=sizes)\n","        self.l2_factor = l2_factor\n","\n","        self.momentum = momentum\n","        self.velocity_w = [np.zeros_like(w) for w in self.weights]\n","        self.velocity_b = [np.zeros_like(b) for b in self.biases]\n","\n","\n","    def learning_step(self, x_mini_batch: FloatNDArray, y_mini_batch: FloatNDArray, learning_rate: float) -> None:\n","        \"\"\"\n","        Update parameters with one mini-batch step of backprop and gradient descent (with momentum and L2-regularization).\n","\n","        Args:\n","        - x_mini_batch: shape (B, N^0) where B is mini_batch_size.\n","        - y_mini_batch: shape (B, N^last).\n","        - learning_rate.\n","        \"\"\"\n","\n","        B = x_mini_batch.shape[0]\n","        grads_w, grads_b = self.backprop(x_mini_batch, y_mini_batch)\n","\n","\n","        # Only L-2 Regularization\n","        # self.weights = [(1 - learning_rate * self.l2_factor / x_mini_batch.shape[0]) * w - learning_rate * grad_w\n","        #     for w, grad_w in zip(self.weights, grads_w, strict=True)]\n","\n","        # self.biases = [b - learning_rate * grad_b\n","        #     for b, grad_b in zip(self.biases, grads_b, strict=True)]\n","\n","        self.velocity_w = [self.momentum * self.velocity_w[i] + grads_w[i] + self.l2_factor * self.weights[i]\n","                           for i in range(len(self.weights))]\n","        self.velocity_b = [self.momentum * self.velocity_b[i] + grads_b[i]\n","                           for i in range(len(self.biases))]\n","\n","        self.weights = [self.weights[i] - learning_rate * self.velocity_w[i]\n","                        for i in range(len(self.weights))]\n","        self.biases = [self.biases[i] - learning_rate * self.velocity_b[i]\n","                       for i in range(len(self.biases))]\n","\n","\n","model_results.evaluate_model(\n","    model_name=f\"L2&Momentum\",\n","    model_constructor=Task2,\n","    learning_rates=[10.0],\n","    n_trainings=1,\n","    l2_factor=1e-7,\n","    momentum=0.3\n",")\n","model_results.display_results()"]},{"cell_type":"code","source":["class Task1And2(Task2, Task1):\n","    # A somewhat hacky but short way to mix Task1 and Task2.\n","    # You could also just replace the superclass of Task2 to be Task1.\n","    pass\n","\n","model_results.evaluate_model(\n","    model_name=f\"Softmax&L2&Momentum\",\n","    model_constructor=Task1And2,\n","    learning_rates=[2.0],\n","    n_trainings=3,\n","    l2_factor=1e-6,\n","    momentum=0.1\n",")\n","model_results.display_results()"],"metadata":{"id":"nnBGG1xo0F34","colab":{"base_uri":"https://localhost:8080/","height":373},"executionInfo":{"status":"ok","timestamp":1761749568793,"user_tz":-60,"elapsed":36501,"user":{"displayName":"Wojciech Mierzejek","userId":"03526477555098188127"}},"outputId":"deeacc12-41c6-4cd6-ba24-0bea6f69daee"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Checking 3 random trainings with with lr = 2.0\n"]},{"output_type":"stream","name":"stderr","text":["Epoch: 100%|██████████| 10/10 [00:11<00:00,  1.16s/it, Test accuracy: 94.47 %]\n","Epoch: 100%|██████████| 10/10 [00:11<00:00,  1.15s/it, Test accuracy: 94.90 %]\n","Epoch: 100%|██████████| 10/10 [00:13<00:00,  1.32s/it, Test accuracy: 94.61 %]\n"]},{"output_type":"display_data","data":{"text/plain":["<pandas.io.formats.style.Styler at 0x7b34f0c44800>"],"text/html":["<style type=\"text/css\">\n","</style>\n","<table id=\"T_25641\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th id=\"T_25641_level0_col0\" class=\"col_heading level0 col0\" >model</th>\n","      <th id=\"T_25641_level0_col1\" class=\"col_heading level0 col1\" >lr</th>\n","      <th id=\"T_25641_level0_col2\" class=\"col_heading level0 col2\" >accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td id=\"T_25641_row0_col0\" class=\"data row0 col0\" >L2&Momentum(l2_factor=1e-07,momentum=0.3)</td>\n","      <td id=\"T_25641_row0_col1\" class=\"data row0 col1\" >1e+01</td>\n","      <td id=\"T_25641_row0_col2\" class=\"data row0 col2\" >95.7% ± 0.0 p.p.</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_25641_row1_col0\" class=\"data row1 col0\" >Baseline</td>\n","      <td id=\"T_25641_row1_col1\" class=\"data row1 col1\" >1e+01</td>\n","      <td id=\"T_25641_row1_col2\" class=\"data row1 col2\" >94.8% ± 0.1 p.p.</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_25641_row2_col0\" class=\"data row2 col0\" >Softmax&L2&Momentum(l2_factor=1e-06,momentum=0.1)</td>\n","      <td id=\"T_25641_row2_col1\" class=\"data row2 col1\" >2</td>\n","      <td id=\"T_25641_row2_col2\" class=\"data row2 col2\" >94.7% ± 0.2 p.p.</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_25641_row3_col0\" class=\"data row3 col0\" >SoftMax</td>\n","      <td id=\"T_25641_row3_col1\" class=\"data row3 col1\" >1</td>\n","      <td id=\"T_25641_row3_col2\" class=\"data row3 col2\" >92.8% ± 0.7 p.p.</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_25641_row4_col0\" class=\"data row4 col0\" >Baseline</td>\n","      <td id=\"T_25641_row4_col1\" class=\"data row4 col1\" >1</td>\n","      <td id=\"T_25641_row4_col2\" class=\"data row4 col2\" >91.0% ± 0.1 p.p.</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_25641_row5_col0\" class=\"data row5 col0\" >SoftMax</td>\n","      <td id=\"T_25641_row5_col1\" class=\"data row5 col1\" >1e+01</td>\n","      <td id=\"T_25641_row5_col2\" class=\"data row5 col2\" >67.7% ± 39.8 p.p.</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_25641_row6_col0\" class=\"data row6 col0\" >SoftMax</td>\n","      <td id=\"T_25641_row6_col1\" class=\"data row6 col1\" >1e+02</td>\n","      <td id=\"T_25641_row6_col2\" class=\"data row6 col2\" >9.7% ± 0.6 p.p.</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_25641_row7_col0\" class=\"data row7 col0\" >Baseline</td>\n","      <td id=\"T_25641_row7_col1\" class=\"data row7 col1\" >1e+02</td>\n","      <td id=\"T_25641_row7_col2\" class=\"data row7 col2\" >9.3% ± 0.6 p.p.</td>\n","    </tr>\n","  </tbody>\n","</table>\n"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"L6nLauKUXywE"},"source":["## Task 3 (optional)\n","Implement more variations of SGD:\n","* AdamW (probably the most popular choice) or Adagrad,\n","* dropout\n","* some simple data augmentations (e.g. tiny rotations/shifts etc.).\n","\n","Again, test to see how these changes improve accuracy/convergence.  \n","\n","Quick reminders:\n","* for AdamW, check the official [PyTorch documentation](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html)'s pseudocode or the original paper: [Decoupled Weight Decay Regularization](https://arxiv.org/abs/1711.05101).\n","* for AdaGrad, check the Appendix of this notebook.\n","* for dropout: during training only, zero-out each activation in the considered layer with probability $p$, and multiplying other activations by $\\frac{1}{1-p}$."]},{"cell_type":"code","execution_count":12,"metadata":{"id":"8X3hRIizXywE","executionInfo":{"status":"ok","timestamp":1761749903051,"user_tz":-60,"elapsed":18035,"user":{"displayName":"Wojciech Mierzejek","userId":"03526477555098188127"}}},"outputs":[],"source":["import torch\n","import torchvision.transforms\n","\n","# def add_tiny_rotations_torch(x_train, angle_range=(-10, 10)):\n","#     transform = T.Compose([\n","#         T.ToPILImage(),\n","#         T.RandomRotation(angle_range),\n","#         T.ToTensor(),\n","#     ])\n","#     imgs = torch.tensor(x_train.reshape(-1, 28, 28))\n","#     x_train_rot = torch.stack([transform(img).view(-1) for img in imgs])\n","#     return x_train_rot.numpy()\n","\n","transform = torchvision.transforms.Compose([\n","    torchvision.transforms.ToPILImage(),\n","    torchvision.transforms.RandomAffine(degrees=10, translate=(0.1, 0.1)),\n","    torchvision.transforms.ToTensor(),\n","])\n","\n","imgs = torch.tensor(x_train.reshape(-1, 28, 28))\n","x_train_aug = torch.stack([transform(img).view(-1) for img in imgs])\n","x_train_aug = x_train_aug.numpy()\n","\n","x_train = np.concatenate((x_train, x_train_aug), axis=0)\n","y_train = np.concatenate((y_train, y_train), axis=0)"]},{"cell_type":"code","source":["model_results.evaluate_model(\n","    model_name=f\"Softmax&L2&&augmentations\",\n","    model_constructor=Task1And2,\n","    learning_rates=[2.0],\n","    n_trainings=3,\n","    l2_factor=1e-6,\n","    momentum=0.1\n",")\n","model_results.display_results()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":404},"id":"cFpgagjjejuV","executionInfo":{"status":"ok","timestamp":1761750014259,"user_tz":-60,"elapsed":66279,"user":{"displayName":"Wojciech Mierzejek","userId":"03526477555098188127"}},"outputId":"d13bbd3b-b68c-4a21-b22f-7b47e8f1e42c"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Checking 3 random trainings with with lr = 2.0\n"]},{"output_type":"stream","name":"stderr","text":["Epoch: 100%|██████████| 10/10 [00:22<00:00,  2.29s/it, Test accuracy: 92.61 %]\n","Epoch: 100%|██████████| 10/10 [00:20<00:00,  2.03s/it, Test accuracy: 92.54 %]\n","Epoch: 100%|██████████| 10/10 [00:22<00:00,  2.29s/it, Test accuracy: 91.86 %]\n"]},{"output_type":"display_data","data":{"text/plain":["<pandas.io.formats.style.Styler at 0x7b34e8057470>"],"text/html":["<style type=\"text/css\">\n","</style>\n","<table id=\"T_ff4d1\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th id=\"T_ff4d1_level0_col0\" class=\"col_heading level0 col0\" >model</th>\n","      <th id=\"T_ff4d1_level0_col1\" class=\"col_heading level0 col1\" >lr</th>\n","      <th id=\"T_ff4d1_level0_col2\" class=\"col_heading level0 col2\" >accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td id=\"T_ff4d1_row0_col0\" class=\"data row0 col0\" >L2&Momentum(l2_factor=1e-07,momentum=0.3)</td>\n","      <td id=\"T_ff4d1_row0_col1\" class=\"data row0 col1\" >1e+01</td>\n","      <td id=\"T_ff4d1_row0_col2\" class=\"data row0 col2\" >95.7% ± 0.0 p.p.</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_ff4d1_row1_col0\" class=\"data row1 col0\" >Baseline</td>\n","      <td id=\"T_ff4d1_row1_col1\" class=\"data row1 col1\" >1e+01</td>\n","      <td id=\"T_ff4d1_row1_col2\" class=\"data row1 col2\" >94.8% ± 0.1 p.p.</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_ff4d1_row2_col0\" class=\"data row2 col0\" >Softmax&L2&Momentum(l2_factor=1e-06,momentum=0.1)</td>\n","      <td id=\"T_ff4d1_row2_col1\" class=\"data row2 col1\" >2</td>\n","      <td id=\"T_ff4d1_row2_col2\" class=\"data row2 col2\" >94.7% ± 0.2 p.p.</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_ff4d1_row3_col0\" class=\"data row3 col0\" >SoftMax</td>\n","      <td id=\"T_ff4d1_row3_col1\" class=\"data row3 col1\" >1</td>\n","      <td id=\"T_ff4d1_row3_col2\" class=\"data row3 col2\" >92.8% ± 0.7 p.p.</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_ff4d1_row4_col0\" class=\"data row4 col0\" >Softmax&L2&&augmentations(l2_factor=1e-06,momentum=0.1)</td>\n","      <td id=\"T_ff4d1_row4_col1\" class=\"data row4 col1\" >2</td>\n","      <td id=\"T_ff4d1_row4_col2\" class=\"data row4 col2\" >92.3% ± 0.3 p.p.</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_ff4d1_row5_col0\" class=\"data row5 col0\" >Baseline</td>\n","      <td id=\"T_ff4d1_row5_col1\" class=\"data row5 col1\" >1</td>\n","      <td id=\"T_ff4d1_row5_col2\" class=\"data row5 col2\" >91.0% ± 0.1 p.p.</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_ff4d1_row6_col0\" class=\"data row6 col0\" >SoftMax</td>\n","      <td id=\"T_ff4d1_row6_col1\" class=\"data row6 col1\" >1e+01</td>\n","      <td id=\"T_ff4d1_row6_col2\" class=\"data row6 col2\" >67.7% ± 39.8 p.p.</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_ff4d1_row7_col0\" class=\"data row7 col0\" >SoftMax</td>\n","      <td id=\"T_ff4d1_row7_col1\" class=\"data row7 col1\" >1e+02</td>\n","      <td id=\"T_ff4d1_row7_col2\" class=\"data row7 col2\" >9.7% ± 0.6 p.p.</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_ff4d1_row8_col0\" class=\"data row8 col0\" >Baseline</td>\n","      <td id=\"T_ff4d1_row8_col1\" class=\"data row8 col1\" >1e+02</td>\n","      <td id=\"T_ff4d1_row8_col2\" class=\"data row8 col2\" >9.3% ± 0.6 p.p.</td>\n","    </tr>\n","  </tbody>\n","</table>\n"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"5HCbwiW2XywE"},"source":["## Task 4\n","Try adding extra layers to the network. Again, test how the changes you introduced affect accuracy/convergence and what learning rates work.\n","\n","As a start, you can try this slightly larger architecture: [784,100,30,10]  \n"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"92OPX1uCXywE","executionInfo":{"status":"ok","timestamp":1761749568829,"user_tz":-60,"elapsed":4,"user":{"displayName":"Wojciech Mierzejek","userId":"03526477555098188127"}}},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"8xl3A1WSXywE"},"source":["# Appendix"]},{"cell_type":"markdown","metadata":{"id":"rRX8ith-XywF"},"source":["## Adagrad (simplified version)\n","\n","Let $p_1, \\ldots, p_n$ be all parameters in our model (weights and biases).  \n","For parameter $p_i$ we maintain a variable $G_i$ (can be set to $0$ initially).\n","Let $\\mathcal{L}$ be our loss without L2.   \n","We update $G_i$ and $p_i$ each training step as follows:  \n","$$\n","G_i = G_i +  \\left(\\frac{\\partial \\mathcal{L}}{\\partial p_i}\\right)^2\\\\\n","p_i = p_i - \\frac{\\eta}{\\sqrt{\\left(G_i + \\epsilon\\right)}}\\frac{\\partial \\mathcal{L}}{\\partial p_i}\n","$$"]},{"cell_type":"code","source":[],"metadata":{"id":"cfBEYR4rYXmy"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/mim-ml-teaching/public-dnn-2025-26/blob/master/docs/DNN-Lab-3-mnist-again-student.ipynb","timestamp":1761142950739}]},"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":0}